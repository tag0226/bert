{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chapter8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON6jU-cos5E1"
      },
      "source": [
        "# 8章\n",
        "- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6r9ATFJImOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b42f01c-1a9d-4b08-aab4-59149849cd4d"
      },
      "source": [
        "# 8-1\n",
        "!mkdir chap8\n",
        "%cd ./chap8"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chap8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hJ-pXOwXBzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac228c5-44d6-4a29-c409-da9a6a920461"
      },
      "source": [
        "# 8-2\n",
        "!pip install transformers fugashi ipadic pytorch-lightning"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting ipadic\n",
            "  Using cached ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Downloading fugashi-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.7/671.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=b26dd90bff5872b212c0194d92dab8d86f0202b4c5c2ebf6155731c6b82a5fd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic, lightning-utilities, fugashi, torchmetrics, pytorch-lightning\n",
            "Successfully installed fugashi-1.4.0 ipadic-1.0.0 lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWsBlMRNhnnx"
      },
      "source": [
        "# 8-3\n",
        "import itertools\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilBB1q6zPvww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1770659-10c9-4926-8e30-6ff81daa7c46"
      },
      "source": [
        "# 8-4\n",
        "normalize = lambda s: unicodedata.normalize(\"NFKC\",s)\n",
        "print(f'ＴＡＧＵＣＨＩ -> {normalize(\"ＴＡＧＵＣＨＩ\")}' )  # 全角アルファベット\n",
        "print(f'taguchi -> {normalize(\"taguchi\")}' )        # 半角アルファベット\n",
        "print(f'０１２６ -> {normalize(\"０１２６\")}' )  # 全角数字\n",
        "print(f'0126 -> {normalize(\"0126\")}' )        # 半角数字\n",
        "print(f'タグチ -> {normalize(\"タグチ\")}' )  # 全角カタカナ\n",
        "print(f'ﾀｸﾞﾁ -> {normalize(\"ﾀｸﾞﾁ\")}' )        # 半角カタカナ"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ＴＡＧＵＣＨＩ -> TAGUCHI\n",
            "taguchi -> taguchi\n",
            "０１２６ -> 0126\n",
            "0126 -> 0126\n",
            "タグチ -> タグチ\n",
            "ﾀｸﾞﾁ -> タグチ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mecab-python3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP0QlDaItfRD",
        "outputId": "73039fca-b24d-4438-8d4f-475a3b0de93a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Downloading mecab_python3-1.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import ipadic\n",
        "\n",
        "tagger = MeCab.Tagger(ipadic.MECAB_ARGS) # ipadic を辞書に指定してインスタンス生成\n",
        "\n",
        "text1 = \"田口君は12月１６日、500円のモンブランを買った。\" # 解析対象のテキストを定義\n",
        "\n",
        "text2 = \"たぐち君は１２月16日、５００円のﾓﾝﾌﾞﾗんを買った。\" # 解析対象のテキストを定義\n",
        "\n",
        "parsed_text1 = tagger.parse(text1) # 解析を実行\n",
        "parsed_text2 = tagger.parse(text2) # 解析を実行\n",
        "print(parsed_text1) # 解析結果を表示\n",
        "print(parsed_text2) # 解析結果を表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8n1te5ZtY4E",
        "outputId": "2169ebf1-76e0-4071-8336-dfba552063fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "田口\t名詞,固有名詞,人名,姓,*,*,田口,タグチ,タグチ\n",
            "君\t名詞,接尾,人名,*,*,*,君,クン,クン\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "12\t名詞,数,*,*,*,*,*\n",
            "月\t名詞,一般,*,*,*,*,月,ツキ,ツキ\n",
            "１\t名詞,数,*,*,*,*,１,イチ,イチ\n",
            "６\t名詞,数,*,*,*,*,６,ロク,ロク\n",
            "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "500\t名詞,数,*,*,*,*,*\n",
            "円\t名詞,接尾,助数詞,*,*,*,円,エン,エン\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "モンブラン\t名詞,固有名詞,地域,一般,*,*,モンブラン,モンブラン,モンブラン\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "買っ\t動詞,自立,*,*,五段・ワ行促音便,連用タ接続,買う,カッ,カッ\n",
            "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n",
            "EOS\n",
            "\n",
            "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
            "ぐち\t名詞,一般,*,*,*,*,ぐち,グチ,グチ\n",
            "君\t名詞,接尾,人名,*,*,*,君,クン,クン\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "１２月\t名詞,副詞可能,*,*,*,*,１２月,ジュウニガツ,ジューニガツ\n",
            "16\t名詞,数,*,*,*,*,*\n",
            "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "５\t名詞,数,*,*,*,*,５,ゴ,ゴ\n",
            "０\t名詞,数,*,*,*,*,０,ゼロ,ゼロ\n",
            "０\t名詞,数,*,*,*,*,０,ゼロ,ゼロ\n",
            "円\t名詞,接尾,助数詞,*,*,*,円,エン,エン\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "ﾓﾝﾌﾞﾗ\t名詞,一般,*,*,*,*,*\n",
            "ん\t名詞,非自立,一般,*,*,*,ん,ン,ン\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "買っ\t動詞,自立,*,*,五段・ワ行促音便,連用タ接続,買う,カッ,カッ\n",
            "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnK2OiMKhmt0"
      },
      "source": [
        "# 8-5\n",
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"\n",
        "        文章とそれに含まれる固有表現が与えられた時に、\n",
        "        符号化とラベル列の作成を行う。\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
        "        splitted = [] # 分割後の文字列を追加していく\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            # 固有表現ではないものには0のラベルを付与\n",
        "            splitted.append({'text':text[position:start], 'label':0})\n",
        "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "            splitted.append({'text':text[start:end], 'label':label})\n",
        "            position = end\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n",
        "\n",
        "        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n",
        "        tokens = [] # トークンを追加していく\n",
        "        labels = [] # トークンのラベルを追加していく\n",
        "        for text_splitted in splitted:\n",
        "            text = text_splitted['text']\n",
        "            label = text_splitted['label']\n",
        "            tokens_splitted = self.tokenize(text)\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        ) # input_idsをencodingに変換\n",
        "        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n",
        "        labels = [0] + labels[:max_length-2] + [0]\n",
        "        # 特殊トークン[PAD]のラベルを0にする。\n",
        "        labels = labels + [0]*( max_length - len(labels) )\n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークンを追加していく。\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length' if max_length else False,\n",
        "            truncation=True if max_length else False\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2]\n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) )\n",
        "\n",
        "        # 必要に応じてtorch.Tensorにする。\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"\n",
        "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "\n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh07iizk9Onf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f518441e-34eb-499a-ea79-8be39cf6276c"
      },
      "source": [
        "# 8-6\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install unidic-lite"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy-ws36crt90",
        "outputId": "ebf270c0-2080-460b-b74b-06829db85538"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidic-lite in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5704VNOVt8Yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1aacfd-bd28-44dc-a624-9d200726c091"
      },
      "source": [
        "# 8-7\n",
        "text = '昨日のみらい事務所との打ち合わせは順調だった。'\n",
        "entities = [\n",
        "    {'name': 'みらい事務所', 'span': [3,9], 'type_id': 1}\n",
        "]\n",
        "\n",
        "encoding = tokenizer.encode_plus_tagged(\n",
        "    text, entities, max_length=20\n",
        ")\n",
        "print(encoding)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [2, 10271, 28486, 5, 546, 3000, 1518, 233, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDik0FRIL38J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b776f0c3-bd47-4a28-a82e-2919bc7984a0"
      },
      "source": [
        "# 8-8\n",
        "text = '騰訊の英語名はTencent Holdings Ltdである。'\n",
        "encoding, spans = tokenizer.encode_plus_untagged(\n",
        "    text, return_tensors='pt'\n",
        ")\n",
        "print('# encoding')\n",
        "print(encoding)\n",
        "print('# spans')\n",
        "print(spans)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# encoding\n",
            "{'input_ids': tensor([[    2,     1,     5,  1543,   125,     9,  6749, 28550,  2953, 28550,\n",
            "         28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "# spans\n",
            "[[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHS30W_woM-E"
      },
      "source": [
        "# 8-9\n",
        "labels_predicted = [0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0]\n",
        "entities = tokenizer.convert_bert_output_to_entities(\n",
        "    text, labels_predicted, spans\n",
        ")\n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo45M23hSL_-"
      },
      "source": [
        "# 8-10\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "bert_tc = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=4\n",
        ")\n",
        "bert_tc = bert_tc.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t344Ak2_0C2c"
      },
      "source": [
        "# 8-11\n",
        "text = 'AさんはB大学に入学した。'\n",
        "\n",
        "# 符号化を行い、各トークンの文章中での位置も特定しておく。\n",
        "encoding, spans = tokenizer.encode_plus_untagged(\n",
        "    text, return_tensors='pt'\n",
        ")\n",
        "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "\n",
        "# BERTでトークン毎の分類スコアを出力し、スコアの最も高いラベルを予測値とする。\n",
        "with torch.no_grad():\n",
        "    output = bert_tc(**encoding)\n",
        "    scores = output.logits\n",
        "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
        "\n",
        "# ラベル列を固有表現に変換\n",
        "entities = tokenizer.convert_bert_output_to_entities(\n",
        "    text, labels_predicted, spans\n",
        ")\n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmFZVpb208CK"
      },
      "source": [
        "# 8-12\n",
        "data = [\n",
        "    {\n",
        "        'text': 'AさんはB大学に入学した。',\n",
        "        'entities': [\n",
        "            {'name': 'A', 'span': [0, 1], 'type_id': 2},\n",
        "            {'name': 'B大学', 'span': [4, 7], 'type_id': 1}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'text': 'CDE株式会社は新製品「E」を販売する。',\n",
        "        'entities': [\n",
        "            {'name': 'CDE株式会社', 'span': [0, 7], 'type_id': 1},\n",
        "            {'name': 'E', 'span': [12, 13], 'type_id': 3}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# 各データを符号化し、データローダを作成する。\n",
        "max_length=32\n",
        "dataset_for_loader = []\n",
        "for sample in data:\n",
        "    text = sample['text']\n",
        "    entities = sample['entities']\n",
        "    encoding = tokenizer.encode_plus_tagged(\n",
        "        text, entities, max_length=max_length\n",
        "    )\n",
        "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
        "    dataset_for_loader.append(encoding)\n",
        "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
        "\n",
        "# ミニバッチを取り出し損失を得る。\n",
        "for batch in dataloader:\n",
        "    batch = { k: v.cuda() for k, v in batch.items() } # GPU\n",
        "    output = bert_tc(**batch) # BERTへ入力\n",
        "    loss = output.loss # 損失"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-4aEmfGf9qT"
      },
      "source": [
        "# 8-13\n",
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpwR_RXZlX80"
      },
      "source": [
        "# 8-14\n",
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付る辞書\n",
        "type_id_dict = {\n",
        "    \"人名\": 1,\n",
        "    \"法人名\": 2,\n",
        "    \"政治的組織名\": 3,\n",
        "    \"その他の組織名\": 4,\n",
        "    \"地名\": 5,\n",
        "    \"施設名\": 6,\n",
        "    \"製品名\": 7,\n",
        "    \"イベント名\": 8\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する。\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample[\"entities\"]:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "random.shuffle(dataset)\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDGDvIPIlX-s"
      },
      "source": [
        "# 8-15\n",
        "def create_dataset(tokenizer, dataset, max_length):\n",
        "    \"\"\"\n",
        "    データセットをデータローダに入力できる形に整形。\n",
        "    \"\"\"\n",
        "    dataset_for_loader = []\n",
        "    for sample in dataset:\n",
        "        text = sample['text']\n",
        "        entities = sample['entities']\n",
        "        encoding = tokenizer.encode_plus_tagged(\n",
        "            text, entities, max_length=max_length\n",
        "        )\n",
        "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
        "        dataset_for_loader.append(encoding)\n",
        "    return dataset_for_loader\n",
        "\n",
        "# トークナイザのロード\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# データセットの作成\n",
        "max_length = 128\n",
        "dataset_train_for_loader = create_dataset(\n",
        "    tokenizer, dataset_train, max_length\n",
        ")\n",
        "dataset_val_for_loader = create_dataset(\n",
        "    tokenizer, dataset_val, max_length\n",
        ")\n",
        "\n",
        "# データローダの作成\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
        ")\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "446u47Os_Arh"
      },
      "source": [
        "# 8-16\n",
        "# PyTorch Lightningのモデル\n",
        "class BertForTokenClassification_pl(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, num_labels, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.bert_tc = BertForTokenClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        output = self.bert_tc(**batch)\n",
        "        loss = output.loss\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        output = self.bert_tc(**batch)\n",
        "        val_loss = output.loss\n",
        "        self.log('val_loss', val_loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "checkpoint = pl.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=1,\n",
        "    save_weights_only=True,\n",
        "    dirpath='model/'\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=5,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# ファインチューニング\n",
        "model = BertForTokenClassification_pl(\n",
        "    MODEL_NAME, num_labels=9, lr=1e-5\n",
        ")\n",
        "trainer.fit(model, dataloader_train, dataloader_val)\n",
        "best_model_path = checkpoint.best_model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OU3oPuxCEWz"
      },
      "source": [
        "# 8-17\n",
        "def predict(text, tokenizer, bert_tc):\n",
        "    \"\"\"\n",
        "    BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = bert_tc(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# トークナイザのロード\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ファインチューニングしたモデルをロードし、GPUにのせる。\n",
        "model = BertForTokenClassification_pl.load_from_checkpoint(\n",
        "    best_model_path\n",
        ")\n",
        "bert_tc = model.bert_tc.cuda()\n",
        "\n",
        "# 固有表現抽出\n",
        "# 注：以下ではコードのわかりやすさのために、1データづつ処理しているが、\n",
        "# バッチ化して処理を行った方が処理時間は短い\n",
        "entities_list = [] # 正解の固有表現を追加していく。\n",
        "entities_predicted_list = [] # 抽出された固有表現を追加していく。\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, bert_tc) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Fu6xJZ29HJ"
      },
      "source": [
        "# 8-18\n",
        "print(\"# 正解\")\n",
        "print(entities_list[0])\n",
        "print(\"# 抽出\")\n",
        "print(entities_predicted_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeZzmNigiDsD"
      },
      "source": [
        "# 8-19\n",
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted \\\n",
        "        in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [\n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "\n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = \\\n",
        "            set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVbxf1FRlYBU"
      },
      "source": [
        "# 8-20\n",
        "print( evaluate_model(entities_list, entities_predicted_list) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD9sFRUu4Z4c"
      },
      "source": [
        "# 8-21\n",
        "class NER_tokenizer_BIO(BertJapaneseTokenizer):\n",
        "\n",
        "    # 初期化時に固有表現のカテゴリーの数`num_entity_type`を\n",
        "    # 受け入れるようにする。\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.num_entity_type = kwargs.pop('num_entity_type')\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"\n",
        "        文章とそれに含まれる固有表現が与えられた時に、\n",
        "        符号化とラベル列の作成を行う。\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        splitted = [] # 分割後の文字列を追加していく\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            splitted.append({'text':text[position:start], 'label':0})\n",
        "            splitted.append({'text':text[start:end], 'label':label})\n",
        "            position = end\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        splitted = [ s for s in splitted if s['text'] ]\n",
        "\n",
        "        # 分割されたそれぞれの文章をトークン化し、ラベルをつける。\n",
        "        tokens = [] # トークンを追加していく\n",
        "        labels = [] # ラベルを追加していく\n",
        "        for s in splitted:\n",
        "            tokens_splitted = self.tokenize(s['text'])\n",
        "            label = s['label']\n",
        "            if label > 0: # 固有表現\n",
        "                # まずトークン全てにI-タグを付与\n",
        "                labels_splitted =  \\\n",
        "                    [ label + self.num_entity_type ] * len(tokens_splitted)\n",
        "                # 先頭のトークンをB-タグにする\n",
        "                labels_splitted[0] = label\n",
        "            else: # それ以外\n",
        "                labels_splitted =  [0] * len(tokens_splitted)\n",
        "\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # ラベルに特殊トークンを追加\n",
        "        labels = [0] + labels[:max_length-2] + [0]\n",
        "        labels = labels + [0]*( max_length - len(labels) )\n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        IO法のトークナイザのencode_plus_untaggedと同じ\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークンを追加していく。\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length' if max_length else False,\n",
        "            truncation=True if max_length else False\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2]\n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) )\n",
        "\n",
        "        # 必要に応じてtorch.Tensorにする。\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "    @staticmethod\n",
        "    def Viterbi(scores_bert, num_entity_type, penalty=10000):\n",
        "        \"\"\"\n",
        "        Viterbiアルゴリズムで最適解を求める。\n",
        "        \"\"\"\n",
        "        m = 2*num_entity_type + 1\n",
        "        penalty_matrix = np.zeros([m, m])\n",
        "        for i in range(m):\n",
        "            for j in range(1+num_entity_type, m):\n",
        "                if not ( (i == j) or (i+num_entity_type == j) ):\n",
        "                    penalty_matrix[i,j] = penalty\n",
        "\n",
        "        path = [ [i] for i in range(m) ]\n",
        "        scores_path = scores_bert[0] - penalty_matrix[0,:]\n",
        "        scores_bert = scores_bert[1:]\n",
        "\n",
        "        for scores in scores_bert:\n",
        "            assert len(scores) == 2*num_entity_type + 1\n",
        "            score_matrix = np.array(scores_path).reshape(-1,1) \\\n",
        "                + np.array(scores).reshape(1,-1) \\\n",
        "                - penalty_matrix\n",
        "            scores_path = score_matrix.max(axis=0)\n",
        "            argmax = score_matrix.argmax(axis=0)\n",
        "            path_new = []\n",
        "            for i, idx in enumerate(argmax):\n",
        "                path_new.append( path[idx] + [i] )\n",
        "            path = path_new\n",
        "\n",
        "        labels_optimal = path[np.argmax(scores_path)]\n",
        "        return labels_optimal\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, scores, spans):\n",
        "        \"\"\"\n",
        "        文章、分類スコア、各トークンの位置から固有表現を得る。\n",
        "        分類スコアはサイズが（系列長、ラベル数）の2次元配列\n",
        "        \"\"\"\n",
        "        assert len(spans) == len(scores)\n",
        "        num_entity_type = self.num_entity_type\n",
        "\n",
        "        # 特殊トークンに対応する部分を取り除く\n",
        "        scores = [score for score, span in zip(scores, spans) if span[0]!=-1]\n",
        "        spans = [span for span in spans if span[0]!=-1]\n",
        "\n",
        "        # Viterbiアルゴリズムでラベルの予測値を決める。\n",
        "        labels = self.Viterbi(scores, num_entity_type)\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "\n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # 固有表現であれば\n",
        "                if 1 <= label <= num_entity_type:\n",
        "                     # ラベルが`B-`ならば、新しいentityを追加\n",
        "                    entity = {\n",
        "                        \"name\": text[start:end],\n",
        "                        \"span\": [start, end],\n",
        "                        \"type_id\": label\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "                else:\n",
        "                    # ラベルが`I-`ならば、直近のentityを更新\n",
        "                    entity['span'][1] = end\n",
        "                    entity['name'] = text[entity['span'][0]:entity['span'][1]]\n",
        "\n",
        "        return entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvyVJCgH4Z6M"
      },
      "source": [
        "# 8-22\n",
        "# トークナイザのロード\n",
        "# 固有表現のカテゴリーの数`num_entity_type`を入力に入れる必要がある。\n",
        "tokenizer = NER_tokenizer_BIO.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_entity_type=8\n",
        ")\n",
        "\n",
        "# データセットの作成\n",
        "max_length = 128\n",
        "dataset_train_for_loader = create_dataset(\n",
        "    tokenizer, dataset_train, max_length\n",
        ")\n",
        "dataset_val_for_loader = create_dataset(\n",
        "    tokenizer, dataset_val, max_length\n",
        ")\n",
        "\n",
        "# データローダの作成\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
        ")\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykfw0rCA4Z9N"
      },
      "source": [
        "# 8-23\n",
        "\n",
        "# ファインチューニング\n",
        "checkpoint = pl.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=1,\n",
        "    save_weights_only=True,\n",
        "    dirpath='model_BIO/'\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=5,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# PyTorch Lightningのモデルのロード\n",
        "num_entity_type = 8\n",
        "num_labels = 2*num_entity_type+1\n",
        "model = BertForTokenClassification_pl(\n",
        "    MODEL_NAME, num_labels=num_labels, lr=1e-5\n",
        ")\n",
        "\n",
        "# ファインチューニング\n",
        "trainer.fit(model, dataloader_train, dataloader_val)\n",
        "best_model_path = checkpoint.best_model_path\n",
        "\n",
        "# 性能評価\n",
        "model = BertForTokenClassification_pl.load_from_checkpoint(\n",
        "    best_model_path\n",
        ")\n",
        "bert_tc = model.bert_tc.cuda()\n",
        "\n",
        "entities_list = [] # 正解の固有表現を追加していく\n",
        "entities_predicted_list = [] # 抽出された固有表現を追加していく\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = bert_tc(**encoding)\n",
        "        scores = output.logits\n",
        "        scores = scores[0].cpu().numpy().tolist()\n",
        "\n",
        "    # 分類スコアを固有表現に変換する\n",
        "    entities_predicted = tokenizer.convert_bert_output_to_entities(\n",
        "        text, scores, spans\n",
        "    )\n",
        "\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )\n",
        "\n",
        "print(evaluate_model(entities_list, entities_predicted_list))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}